{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的运算操作\n",
    "\n",
    "关于张量的运算操作，从下面3个级别上的运算说明\n",
    "\n",
    " + 标量运算\n",
    " + 向量运算\n",
    " + 矩阵运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow版本:2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow版本', tf.__version__, sep=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **一、标量运算**\n",
    "\n",
    "标量运算就是对张量值中的元素进行运算。运算包括加法(+)、减法(-)、乘法(*)、除(/)、取余(%)、取整(//)等。两个张量的值必须具有相同的类型。\n",
    "  \n",
    "  + 如果进行运算的2个张量的维度数字是完全一致的，进行的运算就是对应元素之间的运算；**参见示例1**\n",
    "  \n",
    "  + 如果两个张量的维度数并不是完全一致的，就会应用到广播机制进行运算；广播机制的原理，就是将维度数字为1的维度上的数\"复制\"，从而变成和另一个张量该维度的数字一样的，然后在进行对应元素间的计算。 **参见示例2.1-2.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]]\n",
      "\n",
      " [[12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23.]]], shape=(2, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]], shape=(2, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-1.  0.  1.  2.]\n",
      "  [ 3.  4.  5.  6.]\n",
      "  [ 7.  8.  9. 10.]]\n",
      "\n",
      " [[11. 12. 13. 14.]\n",
      "  [15. 16. 17. 18.]\n",
      "  [19. 20. 21. 22.]]], shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 示例1：维度数字是完全一致的\n",
    "a = tf.constant(tf.range(24).numpy().reshape(2, 3, 4), dtype=tf.float32)\n",
    "b = tf.constant(tf.ones((2, 3, 4))) # 类型默认为tf.float32\n",
    "print(a, b, sep='\\n')\n",
    "print(a-b) # 减法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]]\n",
      "\n",
      " [[12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23.]]], shape=(2, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]]], shape=(2, 3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 1.  2.  3.  4.]\n",
      "  [ 5.  6.  7.  8.]\n",
      "  [ 9. 10. 11. 12.]]\n",
      "\n",
      " [[13. 14. 15. 16.]\n",
      "  [17. 18. 19. 20.]\n",
      "  [21. 22. 23. 24.]]], shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 示例2.1\n",
    "a = tf.constant(tf.range(24).numpy().reshape(2, 3, 4), dtype=tf.float32)\n",
    "b = tf.constant(tf.ones((2, 3, 1))) # 类型默认为tf.float32\n",
    "print(a, b, sep='\\n')\n",
    "print(a+b) # 加法\n",
    "# 应用到广播机制\n",
    "# 其实就等于把b在第3个维度上进行扩展，等于\"复制”4次，变为tf.ones((2, 3, 4)，维度变成(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[13 14 15 16]\n",
      "  [17 18 19 20]\n",
      "  [21 22 23 24]]], shape=(2, 3, 4), dtype=int32)\n",
      "tf.Tensor([[[4 4 4 4]]], shape=(1, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[4 2 1 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]], shape=(2, 3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 示例2.2\n",
    "a = tf.constant(tf.range(1, 25).numpy().reshape(2, 3, 4))\n",
    "b = tf.constant(tf.fill((1, 1, 4), 4)) \n",
    "print(a, b, sep='\\n')\n",
    "print(b//a) # 取整\n",
    "# 应用到广播机制\n",
    "# 其实就等于把b在第1个维度上\"复制”2次，变成tf.fill((2, 1, 4), 4)，然后再第2个维度\"复制\"3次，变成tf.fill((2, 3, 4), 4)，维度变为(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]], shape=(2, 3, 4), dtype=int32)\n",
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[0 0 0 0]\n",
      "  [1 1 1 1]\n",
      "  [2 2 2 2]]\n",
      "\n",
      " [[3 3 3 3]\n",
      "  [4 4 4 4]\n",
      "  [5 5 5 5]]], shape=(2, 3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 示例2.3\n",
    "a = tf.constant(tf.range(24).numpy().reshape(2, 3, 4))\n",
    "b = tf.constant(tf.fill((1, 1), 4)) \n",
    "print(a, b, sep='\\n')\n",
    "print(a//b) # 取整\n",
    "# 应用到广播机制\n",
    "# 其实就等于把b在第1个维度上\"复制”2次，变成tf.fill((2, 1), 4)\n",
    "# 然后在第2个维度\"复制\"3次，变成tf.fill((2, 3), 4)\n",
    "# 然后在第3个维度\"复制\"4次，变成tf.fill((2, 3, 4), 4),维度变为(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[13 14 15 16]\n",
      "  [17 18 19 20]\n",
      "  [21 22 23 24]]], shape=(2, 3, 4), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[4.         2.         1.33333333 1.        ]\n",
      "  [0.8        0.66666667 0.57142857 0.5       ]\n",
      "  [0.44444444 0.4        0.36363636 0.33333333]]\n",
      "\n",
      " [[0.30769231 0.28571429 0.26666667 0.25      ]\n",
      "  [0.23529412 0.22222222 0.21052632 0.2       ]\n",
      "  [0.19047619 0.18181818 0.17391304 0.16666667]]], shape=(2, 3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# 示例2.4\n",
    "a = tf.constant(tf.range(1, 25).numpy().reshape(2, 3, 4))\n",
    "b = tf.constant(4) \n",
    "print(a, b, sep='\\n')\n",
    "print(b/a) # 除法\n",
    "# 应用到广播机制\n",
    "# 其实就等于把b在第1个维度上\"复制”2次，然后再第2个维度\"复制\"3次，然后再第3个维度\"复制\"4次,维度变为(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1 2 3 4]]\n",
      "\n",
      " [[5 6 7 8]]], shape=(2, 1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[1]\n",
      "  [2]\n",
      "  [3]]], shape=(1, 3, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ 1  2  3  4]\n",
      "  [ 2  4  6  8]\n",
      "  [ 3  6  9 12]]\n",
      "\n",
      " [[ 5  6  7  8]\n",
      "  [10 12 14 16]\n",
      "  [15 18 21 24]]], shape=(2, 3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 示例2.5\n",
    "a = tf.constant(tf.range(1, 9).numpy().reshape(2, 1, 4))\n",
    "b = tf.constant(tf.range(1, 4).numpy().reshape(1, 3, 1)) \n",
    "print(a, b, sep='\\n')\n",
    "print(b*a) # 乘法\n",
    "# 应用到广播机制\n",
    "# 其实就等于把a在第2个维度上\"复制”3次，维度变为(2, 3, 4)\n",
    "# 其实就等于把b在第1个维度上\"复制”2次，然后在第3个维度\"复制\"4次,维度变为(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow2也提供了一些函数，可在模块[**tf.math**](https://tensorflow.google.cn/api_docs/python/tf/math)下查找。下面给出机器学习中经常用到的几个函数示例，其他一些机器学习常用的函数也可在[**tf.nn**](https://tensorflow.google.cn/api_docs/python/tf/nn)中找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[0.25 0.5  0.75 1.  ]]], shape=(1, 1, 4), dtype=float32)\n",
      "神经网络中的激活函数示例\n",
      "sigmoid： tf.Tensor([[[0.5621765  0.62245935 0.6791787  0.7310586 ]]], shape=(1, 1, 4), dtype=float32)\n",
      "tanh： tf.Tensor([[[0.24491866 0.46211717 0.635149   0.7615942 ]]], shape=(1, 1, 4), dtype=float32)\n",
      "sinh： tf.Tensor([[[0.25261232 0.5210953  0.8223167  1.1752012 ]]], shape=(1, 1, 4), dtype=float32)\n",
      "relu： tf.Tensor([[[0.25 0.5  0.75 1.  ]]], shape=(1, 1, 4), dtype=float32)\n",
      "selu： tf.Tensor([[[0.26267526 0.5253505  0.78802574 1.050701  ]]], shape=(1, 1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(tf.linspace(2., 8, 4).numpy().reshape(-1, 1, 4), dtype=tf.float32) / tf.constant(8.)\n",
    "print(a)\n",
    "\n",
    "print('神经网络中的激活函数示例')\n",
    "\n",
    "h = tf.math.sigmoid(a)\n",
    "print('sigmoid：', h)\n",
    "\n",
    "h = tf.math.tanh(a)\n",
    "print('tanh：', h)\n",
    "\n",
    "h = tf.math.sinh(a)\n",
    "print('sinh：', h)\n",
    "\n",
    "h = tf.nn.relu(a)\n",
    "print('relu：', h)\n",
    "\n",
    "h = tf.nn.selu(a)\n",
    "print('selu：', h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "其他函数示例\n",
      "tf.Tensor([[2. 4. 6. 8.]], shape=(1, 4), dtype=float32)\n",
      "sqrt： tf.Tensor([[1.4142134 1.9999999 2.4494896 2.8284268]], shape=(1, 4), dtype=float32)\n",
      "tan： tf.Tensor([[ 4. 16. 36. 64.]], shape=(1, 4), dtype=float32)\n",
      "log： tf.Tensor([[0.6931472 1.3862944 1.7917595 2.0794415]], shape=(1, 4), dtype=float32)\n",
      "exp： tf.Tensor([[   7.389056   54.598152  403.4288   2980.958   ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('其他函数示例')\n",
    "a = tf.constant(tf.linspace(2., 8, 4).numpy().reshape(-1, 4), dtype=tf.float32)\n",
    "print(a)\n",
    "\n",
    "h = tf.math.sqrt(a)\n",
    "print('sqrt：', h)\n",
    "\n",
    "h = tf.math.pow(a, 2)\n",
    "print('tan：', h)\n",
    "\n",
    "h = tf.math.log(a)\n",
    "print('log：', h)\n",
    "\n",
    "h = tf.math.exp(a)\n",
    "print('exp：', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **二、向量运算**\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量运算是以张量值中的向量为计算对象进行运算。进行向量运算的函数，有一些名称是以**reduce**开头，也就是降维。在实际的函数中，是否降维是作为函数参数**keepdims**控制的，设置为**True**保持原来维度，此时维度保持不变，只是在计算维度上的数字变为1；默认设置为**False**，不保持原来维度，此时计算的维度消失，也就是降维了。一个参数就是**axis**，该参数的值表示以哪个维度上的向量为对象，如果不设置该参数的值，则默认该张量中的所有值都参加运算，默认值就是**None**。\n",
    "\n",
    "+ tf.math.reduce_sum()：求和\n",
    "+ tf.math.reduce_mean()：均值\n",
    "+ tf.math.reduce_max()：最大值\n",
    "+ tf.math.reduce_min()：最小值\n",
    "+ tf.math.reduce_std()：标准差\n",
    "+ tf.math.reduce_prod()：乘积\n",
    "+ tf.math.reduce_reduce_variance()：方差\n",
    "+ tf.math.reduce_euclidean_norm()：欧氏距离\n",
    "+ 张量值为逻辑类型的函数\n",
    "    + tf.reduce_all()\n",
    "    + tf.math.reduce_any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例数据\n",
      "tf.Tensor(\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]], shape=(2, 2, 3), dtype=int32)\n",
      "保持维度： tf.Tensor([[[479001600]]], shape=(1, 1, 1), dtype=int32)\n",
      "不保持维度： tf.Tensor(479001600, shape=(), dtype=int32)\n",
      "========================================\n",
      "维度0：\n",
      "保持维度：\n",
      "tf.Tensor(\n",
      "[[[ 7 16 27]\n",
      "  [40 55 72]]], shape=(1, 2, 3), dtype=int32)\n",
      "不保持维度：\n",
      "tf.Tensor(\n",
      "[[ 7 16 27]\n",
      " [40 55 72]], shape=(2, 3), dtype=int32)\n",
      "========================================\n",
      "维度1：\n",
      "保持维度：\n",
      "tf.Tensor(\n",
      "[[[  4  10  18]]\n",
      "\n",
      " [[ 70  88 108]]], shape=(2, 1, 3), dtype=int32)\n",
      "不保持维度：\n",
      "tf.Tensor(\n",
      "[[  4  10  18]\n",
      " [ 70  88 108]], shape=(2, 3), dtype=int32)\n",
      "========================================\n",
      "维度2：\n",
      "保持维度：\n",
      "tf.Tensor(\n",
      "[[[   6]\n",
      "  [ 120]]\n",
      "\n",
      " [[ 504]\n",
      "  [1320]]], shape=(2, 2, 1), dtype=int32)\n",
      "不保持维度：\n",
      "tf.Tensor(\n",
      "[[   6  120]\n",
      " [ 504 1320]], shape=(2, 2), dtype=int32)\n",
      "========================================\n",
      "tf.Tensor(False, shape=(), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True]\n",
      " [False]], shape=(2, 1), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# 示例\n",
    "a = tf.constant(tf.range(1, 13).numpy().reshape(2, 2, 3))\n",
    "print('示例数据', a, sep='\\n')\n",
    "print('保持维度：', tf.math.reduce_prod(a, keepdims=True))\n",
    "print('不保持维度：', tf.math.reduce_prod(a))\n",
    "for i in [0, 1, 2]:# 张量a有3个维度\n",
    "    print('==' * 20)\n",
    "    print('维度%s：' % i, '保持维度：', tf.math.reduce_prod(a, axis=i, keepdims=True),  '不保持维度：', tf.math.reduce_prod(a, axis=i), sep='\\n')\n",
    "\n",
    "print('==' * 20)\n",
    "x = tf.constant([[True,  True], [False, False]])\n",
    "print(tf.reduce_all(x))\n",
    "print(tf.math.reduce_any(x, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出几种机器学习模型中会遇到的几个函数示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, tf.Tensor(2, shape=(), dtype=int64)\n",
      "2, tf.Tensor(2, shape=(), dtype=int64)\n",
      "3, tf.Tensor([2 2 0 2 2], shape=(5,), dtype=int64)\n",
      "4, tf.Tensor([2 2 0 2 2], shape=(5,), dtype=int64)\n",
      "5, tf.Tensor([2 2 1], shape=(3,), dtype=int64)\n",
      "6, tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int64)\n",
      "7, tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int64)\n",
      "8, tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int64)\n",
      "9, tf.Tensor(\n",
      "[[0 0]\n",
      " [0 0]], shape=(2, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 最大值索引与最小值索引 \n",
    "A = tf.constant([2,20,30,3,6])  \n",
    "print('1,', tf.math.argmax(A, 0)) #  最大值的索引\n",
    "print('2,', tf.math.argmax(A)) #  最大值的索引\n",
    "\n",
    "B = tf.constant([[2,20,30,3,6],[3,11,16,1,8],[14,45,23,5,27]])\n",
    "print('3,', tf.math.argmax(B))\n",
    "print('4,', tf.math.argmax(B, 0))\n",
    "print('5,', tf.math.argmax(B,1))\n",
    "\n",
    "C = tf.constant(tf.range(1, 13).numpy().reshape(2, 2, 3))\n",
    "print('6,', tf.math.argmin(C)) #  最小值的索引\n",
    "print('7,', tf.math.argmin(C, 0))\n",
    "print('8,', tf.math.argmin(C,1))\n",
    "print('9,', tf.math.argmin(C,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKV2(values=<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
      "array([[30],\n",
      "       [16],\n",
      "       [45]])>, indices=<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
      "array([[2],\n",
      "       [2],\n",
      "       [1]])>)\n",
      "TopKV2(values=<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[30, 20],\n",
      "       [16, 11],\n",
      "       [45, 27]])>, indices=<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[2, 1],\n",
      "       [2, 1],\n",
      "       [1, 4]])>)\n",
      "TopKV2(values=<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[20, 30],\n",
      "       [11, 16],\n",
      "       [27, 45]])>, indices=<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [1, 2],\n",
      "       [4, 1]])>)\n",
      "值： tf.Tensor(\n",
      "[[20 30]\n",
      " [11 16]\n",
      " [27 45]], shape=(3, 2), dtype=int32)\n",
      "索引： tf.Tensor(\n",
      "[[1 2]\n",
      " [1 2]\n",
      " [4 1]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 张量值的排序\n",
    "B = tf.constant([[2,20,30,3,6],[3,11,16,1,8],[14,45,23,5,27]])\n",
    "print(tf.math.top_k(B)) \n",
    "print(tf.math.top_k(B, k=2)) # 默认k=1，选出最大的k个数；默认sorted=True, 返回的值按降序排列；以及返回的值对应的索引\n",
    "\n",
    "top_k = tf.math.top_k(B, k=2, sorted=False)\n",
    "print(top_k)\n",
    "\n",
    "# 获取得到的值和索引\n",
    "print('值：', top_k.values)\n",
    "print('索引：', top_k.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **三、矩阵运算**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵运算就是以张量值中的矩阵为运算对象，在机器学习中经常用到的包括矩阵乘法，矩阵转置，大部分和矩阵有关的运算都在tf.linalg模块中。\n",
    "+ **tf.linalg.matmul**：矩阵乘法，该函数默认的参数下，两个张量的维度必须是一致的，也就是两个要么都是矩阵，要么都是维度相同的n维数据。两个张量的维度数字除最后2个数字以外，前面的必须是相同的，并且该函数中的第一个张量的维度的最后一个数字，必须等于第二个张量的维度的倒数第2个数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]], shape=(4, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 31  34]\n",
      " [112 124]\n",
      " [193 214]\n",
      " [274 304]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#  矩阵张量示例。3=3\n",
    "a = tf.constant(range(12), shape=[4, 3])\n",
    "print(a) # 矩阵\n",
    "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) \n",
    "print(b) # 矩阵\n",
    "c = tf.linalg.matmul(a, b) # 矩阵乘法\n",
    "print(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]\n",
      "  [11 12 13 14 15]\n",
      "  [16 17 18 19 20]]\n",
      "\n",
      " [[21 22 23 24 25]\n",
      "  [26 27 28 29 30]\n",
      "  [31 32 33 34 35]\n",
      "  [36 37 38 39 40]]], shape=(2, 4, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[13 14 15]\n",
      "  [16 17 18]\n",
      "  [19 20 21]\n",
      "  [22 23 24]\n",
      "  [25 26 27]]\n",
      "\n",
      " [[28 29 30]\n",
      "  [31 32 33]\n",
      "  [34 35 36]\n",
      "  [37 38 39]\n",
      "  [40 41 42]]], shape=(2, 5, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ 315  330  345]\n",
      "  [ 790  830  870]\n",
      "  [1265 1330 1395]\n",
      "  [1740 1830 1920]]\n",
      "\n",
      " [[3940 4055 4170]\n",
      "  [4790 4930 5070]\n",
      "  [5640 5805 5970]\n",
      "  [6490 6680 6870]]], shape=(2, 4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 立方体张量示例，2=2， 5=5\n",
    "a = tf.constant(tf.range(1, 41, dtype=tf.int32).numpy().reshape(2, 4, 5))\n",
    "print(a) # 立方体 \n",
    "b = tf.constant(tf.range(13, 43, dtype=tf.int32).numpy().reshape(2, 5, 3)) \n",
    "print(b) # 立方体\n",
    "c = tf.matmul(a, b) # 矩阵乘法\n",
    "print(c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **tf.transpose**：矩阵转置，也就是进行维度的转化，经常用的就是二维矩阵的转置，也就是行列互换。具体的参见第一天张量维度转换中对该函数的解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(x)\n",
    "print(tf.transpose(x, perm=[1, 0]))\n",
    "print(tf.transpose(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
